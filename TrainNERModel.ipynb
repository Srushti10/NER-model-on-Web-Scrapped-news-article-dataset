{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJjxmiapT39I"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph5OUPCRUD5r"
      },
      "source": [
        "# Function to transform input text files into name and it's tag\n",
        "\n",
        "def preprocess_data(filename):\n",
        "    f = open(filename,encoding=\"utf8\",errors='ignore')\n",
        "    split_text = []\n",
        "    sent = []\n",
        "\n",
        "    for row in f:\n",
        "        if len(row)==0 or row.startswith('-DOCSTART') or row[0]==\"\\n\":\n",
        "            if len(sent) > 0:\n",
        "                split_text.append(sent)\n",
        "                sent = []\n",
        "            continue\n",
        "\n",
        "        splits = row.split(' ')\n",
        "\n",
        "        #Appending the token name and class label\n",
        "        sent.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
        "        if len(sent) > 0:\n",
        "            split_text.append(sent)\n",
        "            sent = []\n",
        "    return split_text\n",
        "   \n",
        "\n",
        "\n",
        "trainSet = preprocess_data(\"train.txt\")\n",
        "validationSet = preprocess_data(\"valid.txt\")\n",
        "testSet = preprocess_data(\"test.txt\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6CYW0gyUFfB"
      },
      "source": [
        "#Storing unique class labels\n",
        "\n",
        "classes = set()\n",
        "words = set()\n",
        "# words and labels\n",
        "\n",
        "for data in [trainSet, validationSet, testSet]:\n",
        "  for labeled_text in data:\n",
        "    for word, label in labeled_text:\n",
        "      classes.add(label)\n",
        "      words.add(word.lower())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yznDrkTUHML"
      },
      "source": [
        "# Sort the set to ensure '0' is assigned to 0\n",
        "ordered_classes = sorted(list(classes), key=len)\n",
        "\n",
        "# Create mapping for labels\n",
        "classToIndex = {}\n",
        "for label in ordered_classes:\n",
        "  classToIndex[label] = len(classToIndex)\n",
        "\n",
        "# Storing unique labels index wise in dictionary\n",
        "count_labels = len(classToIndex)\n",
        "labelMapping = {v: k for k, v in classToIndex.items()}\n",
        "\n",
        "# Create mapping for words\n",
        "wordToIndex = {}\n",
        "if len(wordToIndex) == 0:\n",
        "  wordToIndex[\"PADDING_TOKEN\"] = len(wordToIndex)\n",
        "  wordToIndex[\"UNKNOWN_TOKEN\"] = len(wordToIndex)\n",
        "\n",
        "# Storing unique words index wise in  \n",
        "for word in words:\n",
        "  wordToIndex[word] = len(wordToIndex)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic7G0jQbUIfK"
      },
      "source": [
        "#Storing unique word indexes and labels for all rows in dataset\n",
        "\n",
        "def createMatrices(data, wordToIndex, classToIndex):\n",
        "  words = []\n",
        "  labels = []\n",
        "  for splittedTokens in data:\n",
        "     wordIndices = []\n",
        "     labelIndices = []\n",
        "     for word, label in splittedTokens:\n",
        "       if word in wordToIndex:\n",
        "          wordIdx = wordToIndex[word]\n",
        "       elif word.lower() in wordToIndex:\n",
        "          wordIdx = wordToIndex[word.lower()]\n",
        "       else:\n",
        "          wordIdx = wordToIndex['UNKNOWN_TOKEN']\n",
        "\n",
        "       wordIndices.append(wordIdx)\n",
        "       labelIndices.append(classToIndex[label])\n",
        "     words.append(wordIndices)\n",
        "     labels.append(labelIndices)\n",
        "  return words, labels\n",
        "\n",
        "\n",
        "training_tokens, train_labels = createMatrices(trainSet, wordToIndex, classToIndex)\n",
        "valid_tokens, valid_labels = createMatrices(validationSet, wordToIndex, classToIndex)\n",
        "test_tokens, test_labels = createMatrices(testSet, wordToIndex, classToIndex)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le1DTYJYUgJy"
      },
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "max_seq_len = 128\n",
        "EMBEDDING_DIM = 100\n",
        "epochs = 10\n",
        "\n",
        "def padding(words, labels, max_len, padding='post'):\n",
        "  padded_words = pad_sequences(words, max_len,padding='post')\n",
        "  padded_labels = pad_sequences(labels, max_len, padding='post')\n",
        "  return padded_words, padded_labels\n",
        "\n",
        "\n",
        "#converting into 2D array using pad-sequence\n",
        "train_arr, train_labels = padding(training_tokens, train_labels, max_seq_len, padding='post' )\n",
        "valid_arr, valid_labels = padding(valid_tokens, valid_labels, max_seq_len, padding='post' )\n",
        "test_arr, test_labels = padding(test_tokens, test_labels, max_seq_len, padding='post' )\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idSLFzNhUhrC"
      },
      "source": [
        "# Loading glove embeddings\n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "  values = line.strip().split(' ')\n",
        "  word = values[0] # the first entry is the word\n",
        "  coefs = np.asarray(values[1:], dtype='float32') #100d vectors representing the word\n",
        "  embeddings_index[word] = coefs\n",
        "    \n",
        "f.close()\n",
        "embedding_matrix = np.zeros((len(wordToIndex), EMBEDDING_DIM))\n",
        "\n",
        "# Word embeddings for the tokens\n",
        "for word,i in wordToIndex.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN6c5utTUjLk"
      },
      "source": [
        "train_batch_size = 32\n",
        "valid_batch_size = 64\n",
        "test_batch_size = 64\n",
        "\n",
        "#Creating datasets for tf input pipeline\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_arr, train_labels))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_arr, valid_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_arr, test_labels))\n",
        "\n",
        "#Converting datasets into batches\n",
        "shuffled_train_dataset = train_dataset.shuffle(buffer_size=train_arr.shape[0], reshuffle_each_iteration=True)\n",
        "trainBatch = shuffled_train_dataset.batch(train_batch_size, drop_remainder=True)\n",
        "validationBatch = valid_dataset.batch(valid_batch_size, drop_remainder=True)\n",
        "testBatch = test_dataset.batch(test_batch_size, drop_remainder=True)\n",
        "\n",
        "\n",
        "train_pb_max_len = math.ceil(float(len(train_arr))/float(train_batch_size))\n",
        "valid_pb_max_len = math.ceil(float(len(valid_arr))/float(valid_batch_size))\n",
        "test_pb_max_len = math.ceil(float(len(test_arr))/float(test_batch_size))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV7G1dpGUkxT"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class NERModel(tf.keras.Model):\n",
        "    def __init__(self, max_seq_len, embed_input_dim, embed_output_dim, count_labels, weights):\n",
        "        super(NERModel, self).__init__() \n",
        "        self.embedding = layers.Embedding(input_dim=embed_input_dim,output_dim=embed_output_dim, weights=weights,input_length=max_seq_len, trainable=False, mask_zero=True)        \n",
        "\n",
        "        self.bilstm = layers.Bidirectional(layers.LSTM(128,return_sequences=True))\n",
        "        self.dense = layers.Dense(count_labels)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs) # batchsize, max_seq_len, embedding_output_dim\n",
        "        x = self.bilstm(x) #batchsize, max_seq_len, hidden_dim_bilstm\n",
        "        probs = self.dense(x) #batchsize, max_seq_len, count_labels\n",
        "        return probs\n",
        "\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGorwTR0UmT7"
      },
      "source": [
        "#Setting all model parameters for training\n",
        "\n",
        "model = NERModel(max_seq_len=max_seq_len,embed_input_dim=len(wordToIndex), embed_output_dim=100, weights=[embedding_matrix], count_labels=count_labels)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "lossFunt = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "1IRMdcc1UoI7",
        "outputId": "5f966fb3-0e4e-4070-f878-18bfcb94d138"
      },
      "source": [
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "\n",
        "train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "valid_loss_metric = tf.keras.metrics.Mean('valid_loss', dtype=tf.float32)\n",
        "epochs = 5\n",
        "\n",
        "# Training over calculating the loss function\n",
        "def training_trainset(words_batch, labels_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        probs = model(words_batch)\n",
        "        loss = lossFunt(labels_batch, probs)\n",
        "        \n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(list(zip(grads,model.trainable_variables)))\n",
        "    \n",
        "    return loss, probs\n",
        "\n",
        "def training_validationSet(words_batch, labels_batch):\n",
        "    probs = model(words_batch)\n",
        "    loss = lossFunt(labels_batch, probs)\n",
        "    return loss, probs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for words_batch, labels_batch in progress_bar(trainBatch, total=train_pb_max_len):\n",
        "        loss, probs = training_trainset(words_batch, labels_batch)\n",
        "        train_loss_metric(loss)\n",
        "    train_loss_metric.reset_states()\n",
        "    \n",
        "    for words_batch, labels_batch in progress_bar(validationBatch, total=valid_pb_max_len):\n",
        "        loss, probs = training_validationSet(words_batch, labels_batch)\n",
        "        valid_loss_metric.update_state(loss)\n",
        "    valid_loss_metric.reset_states()\n",
        "\n",
        "\n",
        "model.save_weights(\"model_weights\",save_format='tf')\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='6' class='' max='6364' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.09% [6/6364 00:07<2:07:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e959426ae7d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwords_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_pb_max_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_loss_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtrain_loss_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-e959426ae7d8>\u001b[0m in \u001b[0;36mtraining_trainset\u001b[0;34m(words_batch, labels_batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m def _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[0m\u001b[1;32m    130\u001b[0m                        out_grads, skip_input_indices, forward_pass_name_scope):\n\u001b[1;32m    131\u001b[0m   \"\"\"Calls the gradient function of the op.\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXEMO2ZqRWQG",
        "outputId": "1daceba4-cf6e-4403-afdf-1c81aed7e703"
      },
      "source": [
        "type(train_loss_metric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.metrics.Mean"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "t3KpO1GXVjqd",
        "outputId": "0c09e870-6ac2-4304-bd40-982e9ebc71b2"
      },
      "source": [
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "#Loading the model and assigning the trained weights\n",
        "test_model =  NERModel(max_seq_len=max_seq_len, embed_input_dim=len(wordToIndex), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], count_labels=count_labels)\n",
        "test_model.load_weights(\"model_weights\")\n",
        "\n",
        "# Convert numerical to categorical labels\n",
        "def transformLabels(predictions, correct, labelMapping):\n",
        "  predicted = []\n",
        "  for sentence in predictions:\n",
        "    for i in sentence:\n",
        "      predicted.append([labelMapping[item] for item in i ])\n",
        "\n",
        "\n",
        "  actual = []\n",
        "  if correct != None:\n",
        "    for sentence in correct:\n",
        "      for i in sentence:\n",
        "        actual.append([labelMapping[item] for item in i ])\n",
        "  return actual, predicted\n",
        "\n",
        "\n",
        "\n",
        "actualLabels = []\n",
        "predictedLabels = []\n",
        "i = 0\n",
        "\n",
        "final = pd.DataFrame()\n",
        "# Predict labels over test data\n",
        "for words_batch, labels_batch in progress_bar(testBatch, total=test_pb_max_len):\n",
        "  print(testBatch)\n",
        "  final = final.append(words_batch)\n",
        "  probs = test_model(words_batch)\n",
        "  temp1 = tf.nn.softmax(probs)\n",
        "  preds = tf.argmax(temp1, axis=2)\n",
        "  actualLabels.append(np.asarray(labels_batch))\n",
        "  predictedLabels.append(np.asarray(preds))\n",
        "  i = i+1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8e13a11b3a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Loading the model and assigning the trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mNERModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_input_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordToIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_output_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NERModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkyvPKxoa7yM"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "label_correct, label_pred = transformLabels(predictedLabels, actualLabels, labelMapping)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "5xGLzueMXMGp",
        "outputId": "709541ae-33a4-4d47-cfd7-76d88d179b2e"
      },
      "source": [
        "actualLabels = np.vstack(actualLabels)\n",
        "predictedLabels = np.vstack(predictedLabels)\n",
        "performanceReport = classification_report(actualLabels.flatten(), predictedLabels.flatten())\n",
        "performanceReport"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00   5931093\\n           3       0.60      0.48      0.53       257\\n           4       0.76      0.74      0.75      1667\\n           5       0.82      0.47      0.60      1616\\n           6       0.73      0.50      0.59      1661\\n           7       0.58      0.30      0.39       835\\n           8       0.53      0.19      0.28      1156\\n           9       0.75      0.55      0.64       701\\n          10       0.70      0.44      0.54       214\\n\\n    accuracy                           1.00   5939200\\n   macro avg       0.72      0.52      0.59   5939200\\nweighted avg       1.00      1.00      1.00   5939200\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    }
  ]
}